{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 96, kernel_size=(11, 11), stride=(1, 1), padding=(3, 3))\n",
      "  (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=21, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import BasicModule\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(BasicModule.BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, 11,1,3)\n",
    "        self.conv2 = nn.Conv2d(96, 256, 5,1,2)\n",
    "        self.conv3 = nn.Conv2d(256, 384, 3,1,1)\n",
    "        self.conv4 = nn.Conv2d(384, 384, 3,1,1)\n",
    "        self.conv5 = nn.Conv2d(384, 256, 3,1,1)\n",
    "        self.fc1 = nn.Linear(4096, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 21)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (4, 4))\n",
    "        print(\"conv1:\",x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        print(\"conv2:\",x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        print(\"conv3:\",x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        print(\"conv4:\",x.size())\n",
    "        x = F.relu(self.conv5(x))\n",
    "        print(\"conv5:\",x.size())\n",
    "        x = F.max_pool2d(x,(3,3))\n",
    "        print('pooling:',x.size())\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        print(\"reshape:\",x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(\"fc1:\",x.size())\n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(\"fc2:\",x.size())\n",
    "        x = F.softmax(F.relu(self.fc3(x)),dim=1)\n",
    "        print(\"softMax:\",x.size())\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5185, -0.7337, -0.1952,  ...,  0.8977, -0.3933,  0.8205],\n",
      "          [-1.0236, -0.2606,  1.6496,  ..., -0.8749, -0.9945, -1.3620],\n",
      "          [-0.1602, -0.0364,  0.3763,  ...,  2.0337, -0.2547,  2.3037],\n",
      "          ...,\n",
      "          [ 0.7427, -0.9682, -0.1666,  ..., -0.9605,  1.5831,  0.7427],\n",
      "          [-0.9163,  1.3941,  1.1113,  ..., -0.1612,  0.1427,  0.5154],\n",
      "          [-0.8518,  0.6706,  1.0825,  ..., -0.1456,  0.7850,  0.7905]],\n",
      "\n",
      "         [[-0.4504, -0.1169, -0.9786,  ...,  1.7645, -1.5226, -0.1342],\n",
      "          [ 1.1038, -0.3899,  1.2064,  ...,  1.8480,  1.0397,  0.8054],\n",
      "          [-1.4000,  0.1653,  0.6803,  ...,  1.9723, -1.2644,  3.0783],\n",
      "          ...,\n",
      "          [-0.0403,  0.6453,  0.3214,  ...,  0.5569,  1.1192, -2.4620],\n",
      "          [-1.0226,  1.7656,  1.1781,  ...,  1.1009, -1.0565, -2.2408],\n",
      "          [ 0.4555,  1.8976, -0.2036,  ...,  0.6180,  0.2638, -0.8124]],\n",
      "\n",
      "         [[-2.1689, -0.2735,  0.5136,  ...,  1.1034,  0.3839,  0.6829],\n",
      "          [ 2.6697, -0.0761,  0.4590,  ..., -0.6764,  0.3636,  1.5438],\n",
      "          [-2.1854,  1.2068, -1.2866,  ..., -1.0566,  1.2580,  2.1686],\n",
      "          ...,\n",
      "          [-0.9079, -0.9590, -0.6533,  ..., -0.5010,  0.8774, -0.9394],\n",
      "          [-0.1561,  0.1385, -0.3943,  ...,  1.7136,  0.2235,  0.3909],\n",
      "          [ 0.0229,  0.1867, -0.6627,  ...,  0.8814,  0.8134,  0.6845]]]])\n",
      "conv1: torch.Size([1, 96, 55, 55])\n",
      "conv2: torch.Size([1, 256, 27, 27])\n",
      "conv3: torch.Size([1, 384, 13, 13])\n",
      "conv4: torch.Size([1, 384, 13, 13])\n",
      "conv5: torch.Size([1, 256, 13, 13])\n",
      "pooling: torch.Size([1, 256, 4, 4])\n",
      "reshape: torch.Size([1, 4096])\n",
      "fc1: torch.Size([1, 4096])\n",
      "fc2: torch.Size([1, 4096])\n",
      "fc3: torch.Size([1, 21])\n",
      "tensor([[0.0000, 0.0066, 0.0000, 0.0000, 0.0017, 0.0000, 0.0018, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0005, 0.0000, 0.0071, 0.0127, 0.0000, 0.0000, 0.0000,\n",
      "         0.0120, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.autograd.Variable(torch.randn(1,3,227,227))\n",
    "print(input)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
